# âœ… í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import os  # íŒŒì¼ ê²½ë¡œ ë° ë””ë ‰í† ë¦¬ ì‘ì—…ì„ ìœ„í•œ ëª¨ë“ˆ
import time  # ì‹œê°„ ì¸¡ì •ì„ ìœ„í•œ ëª¨ë“ˆ
import sys  # tqdm ì¶œë ¥ ì¡°ì ˆì„ ìœ„í•œ ì‹œìŠ¤í…œ ëª¨ë“ˆ
import torch  # PyTorch ê¸°ë³¸ ëª¨ë“ˆ
import torch.nn as nn  # ì‹ ê²½ë§ ë ˆì´ì–´ ëª¨ë“ˆ
import torch.optim as optim  # ì˜µí‹°ë§ˆì´ì € ëª¨ë“ˆ
import numpy as np  # ìˆ˜ì¹˜ ê³„ì‚° ëª¨ë“ˆ
from torch.utils.data import Dataset, DataLoader, ConcatDataset  # ë°ì´í„°ì…‹ ë° ë¡œë” ê´€ë ¨
from tqdm import tqdm  # í•™ìŠµ ì§„í–‰ ì‹œê°í™”
import torch.nn.functional as F  # softmax ë“± í•¨ìˆ˜ ì œê³µ

# âœ… ë°°ì¹˜ í´ë”ì—ì„œ ìµœëŒ€ max_filesê°œì˜ .npy íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
# - íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš° npy íŒŒì¼ë§Œ ì¶”ì¶œí•˜ì—¬ ë¬´ì‘ìœ„ë¡œ ì„ê³  max_filesê°œ ë°˜í™˜

def get_npy_paths_from_batch(batch_path, max_files=3000):
    if not os.path.exists(batch_path):  # í´ë”ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return []
    files = [os.path.join(batch_path, f) for f in os.listdir(batch_path) if f.endswith('.npy')]  # npy íŒŒì¼ ëª©ë¡
    np.random.shuffle(files)  # ë¬´ì‘ìœ„ ì…”í”Œ
    return files[:max_files]  # ìƒìœ„ max_filesê°œë§Œ ë°˜í™˜

# âœ… npy íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ í…ì„œë¡œ ë³€í™˜í•˜ëŠ” Dataset í´ë˜ìŠ¤
# - ì—ëŸ¬ ë°œìƒ ì‹œ ë‹¤ìŒ ì¸ë±ìŠ¤ë¡œ ìš°íšŒ ë¡œë”©

class SubsetNpyDataset(Dataset):
    def __init__(self, file_paths, label):  # íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ì™€ ë¼ë²¨
        self.file_paths = file_paths
        self.label = label

    def __len__(self):  # ì „ì²´ ìƒ˜í”Œ ê°œìˆ˜ ë°˜í™˜
        return len(self.file_paths)

    def __getitem__(self, idx):  # ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” í…ì„œ ë°˜í™˜
        try:
            npy_array = np.load(self.file_paths[idx])  # (80, 300)
            npy_array = npy_array.T  # (300, 80)
            npy_array = np.clip(npy_array, -80, 30)  # í´ë¦¬í•‘
            npy_array = (npy_array + 80) / 110  # ì •ê·œí™” (0~1)
            tensor = torch.tensor(npy_array, dtype=torch.float32)  # Tensor ë³€í™˜
            return tensor, self.label  # í…ì„œì™€ ë¼ë²¨ ë°˜í™˜
        except Exception as e:
            print(f"âš ï¸ {self.file_paths[idx]} ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨ - {e}")  # ì—ëŸ¬ ì¶œë ¥
            return self.__getitem__((idx + 1) % len(self))  # ë‹¤ìŒ ìƒ˜í”Œë¡œ ìš°íšŒ

# âœ… CNN + LSTM í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì •ì˜
# - CNNìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ í›„ LSTMìœ¼ë¡œ ì‹œê³„ì—´ í•™ìŠµ â†’ FCë¡œ ë¶„ë¥˜

class CNNLSTM(nn.Module):
    def __init__(self, input_channels):
        super(CNNLSTM, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv1d(input_channels, 128, kernel_size=5, padding=2),  # 1D Convolution
            nn.BatchNorm1d(128),  # ë°°ì¹˜ ì •ê·œí™”
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Conv1d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.MaxPool1d(2)
        )
        self.lstm = nn.LSTM(256, 128, num_layers=1, batch_first=True, bidirectional=True)  # ì–‘ë°©í–¥ LSTM
        self.classifier = nn.Sequential(
            nn.Linear(128 * 2, 64),  # LSTM ì–‘ë°©í–¥ ì¶œë ¥
            nn.ReLU(),
            nn.Linear(64, 2)  # ì´ì§„ ë¶„ë¥˜ ì¶œë ¥ì¸µ
        )

    def forward(self, x):
        x = x.permute(0, 2, 1)  # (B, T, F) â†’ (B, F, T)
        x = self.cnn(x)  # CNN ì²˜ë¦¬
        x = x.permute(0, 2, 1)  # (B, F, T) â†’ (B, T, F)
        lstm_out, _ = self.lstm(x)  # LSTM ì²˜ë¦¬
        pooled = torch.mean(lstm_out, dim=1)  # ì‹œê°„ì¶• í‰ê·  í’€ë§
        return self.classifier(pooled)  # ìµœì¢… ë¶„ë¥˜ ê²°ê³¼

# âœ… Threshold ê¸°ë°˜ í‰ê°€ í•¨ìˆ˜ (ì •í™•ë„, FP/FN í¬í•¨)
# - softmax í™•ë¥  ê¸°ë°˜ ì´ì§„ ë¶„ë¥˜ + í˜¼ë™í–‰ë ¬ í†µê³„

def evaluate(loader, threshold=0.8):
    model.eval()  # í‰ê°€ ëª¨ë“œ
    total_loss, correct, total = 0, 0, 0
    false_positive, false_negative = 0, 0

    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
        for mel, label in loader:
            mel, label = mel.to(device), label.to(device)
            outputs = model(mel)  # ëª¨ë¸ ì˜ˆì¸¡
            probs = F.softmax(outputs, dim=1)  # softmax í™•ë¥ 
            pred_probs = probs[:, 1]  # í´ë˜ìŠ¤ 1 í™•ë¥  ì¶”ì¶œ
            preds = (pred_probs >= threshold).long()  # threshold ê¸°ì¤€ ì´ì§„í™”
            loss = criterion(outputs, label)  # ì†ì‹¤ ê³„ì‚°
            total_loss += loss.item()

            for i in range(label.size(0)):
                true = label[i].item()
                pred = preds[i].item()
                if true == 1 and pred == 0:
                    false_positive += 1  # ë”¥ë³´ì´ìŠ¤ ë†“ì¹¨
                elif true == 0 and pred == 1:
                    false_negative += 1  # ì •ìƒ ì˜¤íƒ

            correct += (preds == label).sum().item()  # ì •í™•ë„ ëˆ„ì 
            total += label.size(0)

    acc = 100 * correct / total  # ì •í™•ë„ (%)
    print(f"\nğŸ“Š Threshold í‰ê°€ ê²°ê³¼ (ê¸°ì¤€ í™•ë¥  â‰¥ {threshold}):")
    print(f"âœ… ì •í™•ë„: {acc:.2f}%")
    print(f"ğŸ”º False Positive (ë”¥ë³´ì´ìŠ¤ë¥¼ ì •ìƒìœ¼ë¡œ íŒë‹¨): {false_positive}ê°œ")
    print(f"ğŸ”» False Negative (ì •ìƒì„ ë”¥ë³´ì´ìŠ¤ë¡œ íŒë‹¨): {false_negative}ê°œ")
    return total_loss, acc

# âœ… ê²½ë¡œ ë° ì„¤ì •ê°’ ì •ì˜
train_batch_indices = list(range(15))  # í•™ìŠµìš© ë°°ì¹˜ ì¸ë±ìŠ¤ (0~24)
val_batch_indices = list(range(25, 27))  # ê²€ì¦ìš© ë°°ì¹˜ ì¸ë±ìŠ¤ (25~28)
label_0_root = "G:\\ë‚´ ë“œë¼ì´ë¸Œ\\Capstone\\label_0"  # ì •ìƒ ë°ì´í„° ê²½ë¡œ
label_1_root = "G:\\ë‚´ ë“œë¼ì´ë¸Œ\\Capstone\\label_1"  # ë”¥ë³´ì´ìŠ¤ ë°ì´í„° ê²½ë¡œ
EPOCHS = 3  # ì—í­ ìˆ˜
BATCH_SIZE = 16  # ë°°ì¹˜ í¬ê¸°
MAX_FILES_PER_CLASS = 2000  # í•™ìŠµìš© ìµœëŒ€ íŒŒì¼ ìˆ˜
VAL_MAX_FILES_PER_CLASS = 500  # ê²€ì¦ìš© ìµœëŒ€ íŒŒì¼ ìˆ˜
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # ë””ë°”ì´ìŠ¤ ì„¤ì •

# âœ… ê²€ì¦ ë°ì´í„°ì…‹ ê³ ì • êµ¬ì„± (ë§¤ epoch ì¬ì‚¬ìš©)
val_paths = []
for idx in val_batch_indices:
    normal_batch = os.path.join(label_0_root, f'batch_{idx:03d}')  # ì •ìƒ ë°°ì¹˜ ê²½ë¡œ
    abnormal_batch = os.path.join(label_1_root, f'batch_{idx:03d}')  # ë¹„ì •ìƒ ë°°ì¹˜ ê²½ë¡œ
    normal_paths = get_npy_paths_from_batch(normal_batch, max_files=VAL_MAX_FILES_PER_CLASS)
    abnormal_paths = get_npy_paths_from_batch(abnormal_batch, max_files=VAL_MAX_FILES_PER_CLASS)
    val_paths.extend([(p, 0) for p in normal_paths])  # ì •ìƒ ë¼ë²¨
    val_paths.extend([(p, 1) for p in abnormal_paths])  # ë”¥ë³´ì´ìŠ¤ ë¼ë²¨
val_dataset_fixed = ConcatDataset([SubsetNpyDataset([p], label) for p, label in val_paths])  # ê²€ì¦ ë°ì´í„°ì…‹ êµ¬ì„±
val_loader = DataLoader(val_dataset_fixed, batch_size=BATCH_SIZE, shuffle=False)  # ê²€ì¦ ë¡œë”

# âœ… ëª¨ë¸ ì´ˆê¸°í™” ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
model = CNNLSTM(input_channels=80).to(device)
criterion = nn.CrossEntropyLoss()  # ì†ì‹¤ í•¨ìˆ˜: ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜
optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Adam ì˜µí‹°ë§ˆì´ì €

# âœ… í•™ìŠµ ë£¨í”„ ì‹œì‘
for epoch in range(EPOCHS):
    print(f"\nğŸ“˜ Epoch {epoch+1}/{EPOCHS}")
    start_time = time.time()  # ì‹œê°„ ì¸¡ì • ì‹œì‘

    # ğŸ” í•™ìŠµìš© ë°ì´í„° ë¡œë”© (ë§¤ epochë§ˆë‹¤ ìƒˆë¡œ ìƒ˜í”Œë§)
    all_paths = []
    for idx in train_batch_indices:
        normal_batch = os.path.join(label_0_root, f'batch_{idx:03d}')
        abnormal_batch = os.path.join(label_1_root, f'batch_{idx:03d}')
        normal_paths = get_npy_paths_from_batch(normal_batch, max_files=MAX_FILES_PER_CLASS)
        abnormal_paths = get_npy_paths_from_batch(abnormal_batch, max_files=MAX_FILES_PER_CLASS)
        all_paths.extend([(p, 0) for p in normal_paths])  # ì •ìƒ
        all_paths.extend([(p, 1) for p in abnormal_paths])  # ë¹„ì •ìƒ

    full_dataset = ConcatDataset([SubsetNpyDataset([p], label) for p, label in all_paths])  # í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì„±
    train_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True)  # í•™ìŠµìš© ë°ì´í„° ë¡œë”

    # ğŸ§  ëª¨ë¸ í•™ìŠµ
    model.train()
    train_loss, train_correct, total = 0, 0, 0
    for mel, label in tqdm(train_loader, desc=f"Epoch {epoch+1} ì§„í–‰ì¤‘", ncols=100, leave=True, dynamic_ncols=True, file=sys.stdout, ascii=True):
        mel, label = mel.to(device), label.to(device)  # GPU ì „ì†¡
        outputs = model(mel)  # ì˜ˆì¸¡
        loss = criterion(outputs, label)  # ì†ì‹¤ ê³„ì‚°
        optimizer.zero_grad()  # ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        loss.backward()  # ì—­ì „íŒŒ
        optimizer.step()  # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        train_loss += loss.item()  # ì†ì‹¤ ëˆ„ì 
        _, pred = torch.max(outputs, 1)  # ì˜ˆì¸¡ ê²°ê³¼
        train_correct += (pred == label).sum().item()  # ì •í™•ë„ ëˆ„ì 
        total += label.size(0)  # ì „ì²´ ìˆ˜

    # ğŸ“ˆ í‰ê°€ ë° ê²°ê³¼ ì¶œë ¥
    elapsed = time.time() - start_time  # ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
    train_acc = 100 * train_correct / total  # í›ˆë ¨ ì •í™•ë„
    val_loss, val_acc = evaluate(val_loader)  # ê²€ì¦ ìˆ˜í–‰
    print(f"ğŸ“Š ê²°ê³¼ - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
    print(f"â±ï¸ Epoch {epoch+1} í•™ìŠµ ì‹œê°„: {elapsed:.2f}ì´ˆ")

# âœ… ëª¨ë¸ ì €ì¥
save_path = "G:\\ë‚´ ë“œë¼ì´ë¸Œ\\Capstone\\modelWithThreshold.pt"
torch.save(model.state_dict(), save_path)  # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì €ì¥
print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
